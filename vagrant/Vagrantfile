# -*- mode: ruby -*-
# vi: set ft=ruby :

# virsh net-list --all
# virsh net-update vagrant-libvirt add-last ip-dhcp-host '<host mac="52:54:00:00:00:01" ip="192.168.121.100"/>' --live --config --parent-index 0
# virsh net-dumpxml vagrant-libvirt

# https://www.rubydoc.info/gems/vagrant-libvirt/0.0.28

ENV["VAGRANT_DEFAULT_PROVIDER"] = "libvirt"

# Charger la configuration d'environnement
# Par défaut: dev, peut être surchargé via ENV['K8S_ENV']
env = ENV['K8S_ENV'] || 'dev'
# LoadBalancer provider (metallb or cilium) - passed from Makefile
lb_provider = ENV['LB_PROVIDER'] || 'metallb'
# CNI primary provider (cilium or calico) - passed from Makefile or env
cni_primary = ENV['CNI_PRIMARY'] || 'cilium'
# Gateway API provider (cilium, traefik, istio, etc.) - passed from Makefile or env
gateway_api_provider = ENV['GATEWAY_API_PROVIDER'] || 'traefik'
# LoadBalancer mode (l2 or bgp) - controls FRR VM creation for pure BGP mode
lb_mode = ENV['LB_MODE'] || 'l2'
config_file = File.join(File.dirname(__FILE__), 'config', "#{env}.rb")

unless File.exist?(config_file)
  raise "Configuration file not found: #{config_file}\nAvailable environments: dev, prod"
end

# Charger la configuration de l'environnement
load config_file

Vagrant.configure("2") do |config|
  # Configuration NFS pour Ubuntu 24.04 (NFSv4 + TCP)
  # Mount parent directory to access deploy/argocd/config/config.yaml
  config.vm.synced_folder "..", "/vagrant",
    type: "nfs",
    nfs_version: 4,
    nfs_udp: false

  # Timeout for VM boot (default: 300)
  config.vm.boot_timeout = 600

  # Configuration du management node (optionnel)
  if $management
    config.vm.define "k8s-management" do |management|
      management.vm.box = $vm_box
      management.vm.box_check_update = $box_check_update
      management.vm.hostname = "k8s-management"
      management.vm.network "private_network", ip: "#{$network_prefix}.10"

      management.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = $management_cpu
        domain.memory = $management_memory
        domain.machine_virtual_size = $management_disk
        domain.keymap = $keymap
        # Nested virtualization for Kata Containers
        domain.nested = true
        domain.cpu_mode = "host-passthrough"
      end

      management.vm.provision "shell", path: "scripts/install_common.sh", args: ["management"]
      management.vm.provision "shell", path: "scripts/install_management.sh"
    end
  end

  # LoxiLB external mode: dedicated VM to avoid eBPF conflicts with Cilium
  if lb_provider == 'loxilb'
    config.vm.define "k8s-#{$cluster_name}-loxilb" do |loxilb|
      loxilb.vm.box = $vm_box
      loxilb.vm.box_check_update = $box_check_update
      loxilb.vm.hostname = "k8s-#{$cluster_name}-loxilb"
      loxilb.vm.network "private_network", ip: "#{$network_prefix}.40"

      loxilb.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = 2
        domain.memory = 2048
        domain.machine_virtual_size = 10
        domain.keymap = $keymap
      end

      loxilb.vm.provision "shell", path: "scripts/install_common.sh", args: ["loxilb"]
      loxilb.vm.provision "shell", path: "../deploy/argocd/apps/loxilb/vagrant/provision-loxilb-external.sh", env: {"K8S_ENV" => env}
    end
  end

  # FRR BGP router VM: upstream router for pure BGP mode (no GARP, VIPs via BGP)
  # Created when lb_provider=loxilb and lb_mode=bgp to simulate physical BGP routers.
  # Topology: loxilb (65002) <-eBGP-> FRR (65000) <-eBGP-> Cilium (64512)
  if lb_provider == 'loxilb' && lb_mode == 'bgp'
    config.vm.define "k8s-#{$cluster_name}-frr" do |frr|
      frr.vm.box = $vm_box
      frr.vm.box_check_update = $box_check_update
      frr.vm.hostname = "k8s-#{$cluster_name}-frr"
      frr.vm.network "private_network", ip: "#{$network_prefix}.45"

      frr.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = 1
        domain.memory = 512
        domain.machine_virtual_size = 10
        domain.keymap = $keymap
      end

      frr.vm.provision "shell", path: "scripts/install_common.sh", args: ["frr"]
      frr.vm.provision "shell",
        path: "../deploy/argocd/apps/frr/vagrant/provision-frr.sh",
        env: {"K8S_ENV" => env}
    end
  end

  # Configuration des masters
  (1..$masters).each do |i|
    config.vm.define "k8s-#{$cluster_name}-m#{i}" do |master|
      master.vm.box = $vm_box
      master.vm.box_check_update = $box_check_update
      master.vm.hostname = "k8s-#{$cluster_name}-m#{i}"
      # IP fixe dans la plage .50-.59 (évite conflit avec MetalLB .200-.250)
      master.vm.network "private_network", ip: "#{$network_prefix}.#{50 + i - 1}"

      master.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = $master_cpu
        domain.memory = $master_memory
        domain.machine_virtual_size = $master_disk
        domain.keymap = $keymap
        # Nested virtualization for Kata Containers
        domain.nested = true
        domain.cpu_mode = "host-passthrough"

        # Disque de stockage sur masters en mode all-in-one (pas de workers)
        if $workers == 0 && defined?($storage_disk_enabled) && $storage_disk_enabled
          domain.storage :file,
            :size => $storage_disk_size,
            :device => 'vdb',
            :bus => 'virtio',
            :type => 'raw'
        end
      end

      # Premier master: initialisation du cluster
      if i == 1
        master.vm.provision "shell", inline: <<-SHELL
          rm -f /vagrant/vagrant/{k8s-token,ip_master,kube.config}
          ip a show dev eth1 | egrep -w inet | awk '{ print $2 }' | awk -F'/' '{ print $1 }' > /vagrant/vagrant/ip_master
        SHELL
      end

      # Masters suivants: attente du premier master
      if i > 1
        master.vm.provision "shell", inline: <<-SHELL
          echo "Waiting 1st master node to finish his installation"
          while true
          do
            sleep 5
            test -f /vagrant/vagrant/k8s-token && break
          done
          echo "1st master node has finished his installation. This node can continue his installation"
          mkdir -p /etc/rancher/rke2
          echo "server: https://$(cat /vagrant/vagrant/ip_master):9345" >> /etc/rancher/rke2/config.yaml
          echo "token: $(cat /vagrant/vagrant/k8s-token)" >> /etc/rancher/rke2/config.yaml
        SHELL
      end

      # Masters avec workers séparés: ajouter taint
      if $masters > 1 and $workers > 0
        master.vm.provision "shell", inline: <<-SHELL
          echo "node-taint:" >> /etc/rancher/rke2/config.yaml
          echo '  - "CriticalAddonsOnly=true:NoExecute"' >> /etc/rancher/rke2/config.yaml
        SHELL
      end

      master.vm.provision "shell", path: "scripts/install_common.sh"
      master.vm.provision "shell", path: "scripts/install_master.sh", env: {"CNI_PRIMARY" => cni_primary, "LB_PROVIDER" => lb_provider, "GATEWAY_API_PROVIDER" => gateway_api_provider}

      # Premier master: export kubeconfig et token
      if i == 1
        master.vm.provision "shell", inline: <<-SHELL
          if test -f /etc/rancher/rke2/rke2.yaml
          then
            cp /etc/rancher/rke2/rke2.yaml /vagrant/vagrant/kube.config
            #{if defined?($api_server_ip) && $api_server_ip
              "sed -i \"s@127.0.0.1@#{$api_server_ip}@g\" /vagrant/vagrant/kube.config"
            else
              "sed -i \"s@127.0.0.1@$(cat /vagrant/vagrant/ip_master)@g\" /vagrant/vagrant/kube.config"
            end}
            # sed -i "s@127.0.0.1@k8s-api.k8s.lan@g" /vagrant/vagrant/kube.config
            cp /var/lib/rancher/rke2/server/node-token /vagrant/vagrant/k8s-token
          fi
        SHELL
      end
    end
  end

  # Configuration des workers
  (1..$workers).each do |i|
    config.vm.define "k8s-#{$cluster_name}-w#{i}" do |worker|
      worker.vm.box = $vm_box
      worker.vm.box_check_update = $box_check_update
      worker.vm.hostname = "k8s-#{$cluster_name}-w#{i}"
      # IP fixe dans la plage .60-.99 (évite conflit avec MetalLB .200-.250)
      worker.vm.network "private_network", ip: "#{$network_prefix}.#{60 + i - 1}"

      worker.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = $worker_cpu
        domain.memory = $worker_memory
        domain.machine_virtual_size = $worker_disk
        domain.keymap = $keymap
        # Nested virtualization for Kata Containers
        domain.nested = true
        domain.cpu_mode = "host-passthrough"

        # Disque de stockage supplémentaire (pour Longhorn)
        if defined?($storage_disk_enabled) && $storage_disk_enabled
          domain.storage :file,
            :size => $storage_disk_size,
            :device => 'vdb',
            :bus => 'virtio',
            :type => 'raw'
        end
      end

      worker.vm.provision "shell", path: "scripts/install_common.sh"
      worker.vm.provision "shell", path: "scripts/install_worker.sh"
    end
  end
end
