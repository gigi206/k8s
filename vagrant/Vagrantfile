# -*- mode: ruby -*-
# vi: set ft=ruby :

# virsh net-list --all
# virsh net-update vagrant-libvirt add-last ip-dhcp-host '<host mac="52:54:00:00:00:01" ip="192.168.121.100"/>' --live --config --parent-index 0
# virsh net-dumpxml vagrant-libvirt

# https://www.rubydoc.info/gems/vagrant-libvirt/0.0.28

ENV["VAGRANT_DEFAULT_PROVIDER"] = "libvirt"

# Charger la configuration d'environnement
# Par défaut: dev, peut être surchargé via ENV['K8S_ENV']
env = ENV['K8S_ENV'] || 'dev'
# LoadBalancer provider (metallb or cilium) - passed from Makefile
lb_provider = ENV['LB_PROVIDER'] || 'metallb'
# CNI primary provider (cilium or calico) - passed from Makefile or env
cni_primary = ENV['CNI_PRIMARY'] || 'cilium'
# Gateway API provider (cilium, traefik, istio, etc.) - passed from Makefile or env
gateway_api_provider = ENV['GATEWAY_API_PROVIDER'] || 'traefik'
# LoadBalancer mode (l2 or bgp) - controls FRR VM creation for pure BGP mode
lb_mode = ENV['LB_MODE'] || 'l2'
config_file = File.join(File.dirname(__FILE__), 'config', "#{env}.rb")

unless File.exist?(config_file)
  raise "Configuration file not found: #{config_file}\nAvailable environments: dev, prod"
end

# Charger la configuration de l'environnement
load config_file

Vagrant.configure("2") do |config|
  # Restrict vagrant-libvirt DHCP range to .100-.254 to avoid conflicts
  # with static data IPs (.10-.60) and VRRP VIP (.44).
  # The management network (eth0) gets DHCP on the same subnet as data (eth1),
  # so without this restriction DHCP can assign IPs that conflict with
  # statically configured addresses or virtual IPs.
  config.trigger.before [:up, :reload] do |trigger|
    trigger.name = "Restrict DHCP range on vagrant-libvirt"
    trigger.ruby do |env, machine|
      next if defined?(@dhcp_range_checked)
      @dhcp_range_checked = true
      net_xml = `virsh net-dumpxml vagrant-libvirt 2>/dev/null`
      if net_xml.include?('end=\'192.168.121.254\'') && !net_xml.include?('start=\'192.168.121.100\'')
        system("virsh net-update vagrant-libvirt delete ip-dhcp-range " \
               "'<range start=\"192.168.121.1\" end=\"192.168.121.254\"/>' " \
               "--live --config --parent-index 0 2>/dev/null")
        system("virsh net-update vagrant-libvirt add-last ip-dhcp-range " \
               "'<range start=\"192.168.121.100\" end=\"192.168.121.254\"/>' " \
               "--live --config --parent-index 0 2>/dev/null")
        puts "  DHCP range restricted to .100-.254"
      end
    end
  end

  # Configuration NFS pour Ubuntu 24.04 (NFSv4 + TCP)
  # Mount parent directory to access deploy/argocd/config/config.yaml
  config.vm.synced_folder "..", "/vagrant",
    type: "nfs",
    nfs_version: 4,
    nfs_udp: false

  # Timeout for VM boot (default: 300)
  config.vm.boot_timeout = 600

  # Configuration du management node (optionnel)
  if $management
    config.vm.define "k8s-management" do |management|
      management.vm.box = $vm_box
      management.vm.box_check_update = $box_check_update
      management.vm.hostname = "k8s-management"
      management.vm.network "private_network", ip: "#{$network_prefix}.10"

      management.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = $management_cpu
        domain.memory = $management_memory
        domain.machine_virtual_size = $management_disk
        domain.keymap = $keymap
        # Nested virtualization for Kata Containers
        domain.nested = true
        domain.cpu_mode = "host-passthrough"
      end

      management.vm.provision "shell", path: "scripts/install_common.sh", args: ["management"]
      management.vm.provision "shell", path: "scripts/install_management.sh"
    end
  end

  # LoxiLB external mode: dedicated VM(s) to avoid eBPF conflicts with Cilium
  # Number of VMs derived from loxilb.loxiURL list in deploy/argocd/apps/loxilb/config/<env>.yaml.
  # IPs: $network_prefix.40 (1st), .41 (2nd), .42 (3rd), ...
  if lb_provider == 'loxilb'
    require 'yaml'
    loxilb_app_config = File.join(File.dirname(__FILE__), '..', 'deploy', 'argocd', 'apps', 'loxilb', 'config', "#{env}.yaml")
    loxilb_count = 1
    if File.exist?(loxilb_app_config)
      cfg  = YAML.safe_load(File.read(loxilb_app_config))
      urls = cfg.dig('loxilb', 'loxiURL')
      loxilb_count = urls.is_a?(Array) ? urls.length : 1
    end

    (1..loxilb_count).each do |i|
      vm_ip   = "#{$network_prefix}.#{39 + i}"
      vm_name = "k8s-#{$cluster_name}-loxilb#{i}"

      config.vm.define vm_name do |loxilb|
        loxilb.vm.box              = $vm_box
        loxilb.vm.box_check_update = $box_check_update
        loxilb.vm.hostname         = vm_name
        loxilb.vm.network "private_network", ip: vm_ip

        loxilb.vm.provider "libvirt" do |domain|
          domain.default_prefix          = ""
          domain.cpus                    = 2
          domain.memory                  = 2048
          domain.machine_virtual_size    = 10
          domain.keymap                  = $keymap
        end

        loxilb.vm.provision "shell", path: "scripts/install_common.sh", args: ["loxilb"]
        loxilb.vm.provision "shell",
          path: "../deploy/argocd/apps/loxilb/vagrant/provision-loxilb-external.sh",
          env: {"K8S_ENV" => env}

        # Disable unknown-unicast flooding on bridge ports to prevent eBPF
        # feedback loop when a BGP peer VM goes down (see scripts/configure-loxilb-bridge-ports.sh)
        # Only needed with multiple loxilb instances (feedback loop) + FRR BGP peers (dead peer MAC)
        if loxilb_count > 1 && lb_mode == 'bgp'
          loxilb.trigger.after [:up, :reload] do |trigger|
            trigger.name = "Configure loxilb bridge ports (flood off)"
            trigger.run = {
              inline: "sudo #{File.join(File.dirname(__FILE__), 'scripts', 'configure-loxilb-bridge-ports.sh')} #{vm_name}"
            }
          end
        end

      end
    end
  end

  # FRR BGP router VM(s): upstream router for pure BGP mode (no GARP, VIPs via BGP)
  # Created when lb_mode=bgp for any provider except klipper (klipper uses node IPs, no VIP BGP).
  # Number of VMs and their IPs derived from features.loadBalancer.bgp.peers in
  # deploy/argocd/config/config.yaml — the same list used by the FRR ApplicationSet.
  # Topology depends on provider:
  #   loxilb:  loxilb (65002) <-eBGP-> FRR (65000) <-eBGP-> Cilium (64512)
  #   metallb: metallb-speaker (64512) <-eBGP-> FRR (65000)
  #   cilium:  cilium bgpControlPlane (64512) <-eBGP-> FRR (65000)
  if lb_mode == 'bgp' && lb_provider != 'klipper'
    require 'yaml'
    global_config = File.join(File.dirname(__FILE__), '..', 'deploy', 'argocd', 'config', 'config.yaml')
    frr_peers = [{'address' => "#{$network_prefix}.45"}]  # fallback
    if File.exist?(global_config)
      cfg   = YAML.safe_load(File.read(global_config))
      peers = cfg.dig('features', 'loadBalancer', 'bgp', 'peers')
      frr_peers = peers if peers.is_a?(Array) && !peers.empty?
    end

    frr_peers.each_with_index do |peer, idx|
      vm_name   = "k8s-#{$cluster_name}-frr#{idx + 1}"

      config.vm.define vm_name do |frr|
        frr.vm.box              = $vm_box
        frr.vm.box_check_update = $box_check_update
        frr.vm.hostname         = vm_name
        frr.vm.network "private_network", ip: peer['address']

        frr.vm.provider "libvirt" do |domain|
          domain.default_prefix       = ""
          domain.cpus                 = 1
          domain.memory               = 512
          domain.machine_virtual_size = 10
          domain.keymap               = $keymap
        end

        frr.vm.provision "shell", path: "scripts/install_common.sh", args: ["frr"]
        frr.vm.provision "shell",
          path: "../deploy/argocd/apps/frr/vagrant/provision-frr.sh",
          env: {"K8S_ENV" => env}
      end
    end
  end

  # Configuration des masters
  (1..$masters).each do |i|
    config.vm.define "k8s-#{$cluster_name}-m#{i}" do |master|
      master.vm.box = $vm_box
      master.vm.box_check_update = $box_check_update
      master.vm.hostname = "k8s-#{$cluster_name}-m#{i}"
      # IP fixe dans la plage .50-.59 (évite conflit avec MetalLB .200-.250)
      master.vm.network "private_network", ip: "#{$network_prefix}.#{50 + i - 1}"

      master.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = $master_cpu
        domain.memory = $master_memory
        domain.machine_virtual_size = $master_disk
        domain.keymap = $keymap
        # Nested virtualization for Kata Containers
        domain.nested = true
        domain.cpu_mode = "host-passthrough"

        # Disque de stockage sur masters en mode all-in-one (pas de workers)
        if $workers == 0 && defined?($storage_disk_enabled) && $storage_disk_enabled
          domain.storage :file,
            :size => $storage_disk_size,
            :device => 'vdb',
            :bus => 'virtio',
            :type => 'raw'
        end
      end

      # Premier master: initialisation du cluster
      if i == 1
        master.vm.provision "shell", inline: <<-SHELL
          rm -f /vagrant/vagrant/{k8s-token,ip_master,kube.config}
          ip a show dev eth1 | egrep -w inet | awk '{ print $2 }' | awk -F'/' '{ print $1 }' > /vagrant/vagrant/ip_master
        SHELL
      end

      # Masters suivants: attente du premier master
      if i > 1
        master.vm.provision "shell", inline: <<-SHELL
          echo "Waiting 1st master node to finish his installation"
          while true
          do
            sleep 5
            test -f /vagrant/vagrant/k8s-token && break
          done
          echo "1st master node has finished his installation. This node can continue his installation"
          mkdir -p /etc/rancher/rke2
          echo "server: https://$(cat /vagrant/vagrant/ip_master):9345" >> /etc/rancher/rke2/config.yaml
          echo "token: $(cat /vagrant/vagrant/k8s-token)" >> /etc/rancher/rke2/config.yaml
        SHELL
      end

      # Masters avec workers séparés: ajouter taint
      if $masters > 1 and $workers > 0
        master.vm.provision "shell", inline: <<-SHELL
          echo "node-taint:" >> /etc/rancher/rke2/config.yaml
          echo '  - "CriticalAddonsOnly=true:NoExecute"' >> /etc/rancher/rke2/config.yaml
        SHELL
      end

      master.vm.provision "shell", path: "scripts/install_common.sh"
      master.vm.provision "shell", path: "scripts/install_master.sh", env: {"CNI_PRIMARY" => cni_primary, "LB_PROVIDER" => lb_provider, "LB_MODE" => lb_mode, "GATEWAY_API_PROVIDER" => gateway_api_provider}

      # Premier master: export kubeconfig et token
      if i == 1
        master.vm.provision "shell", inline: <<-SHELL
          if test -f /etc/rancher/rke2/rke2.yaml
          then
            cp /etc/rancher/rke2/rke2.yaml /vagrant/vagrant/kube.config
            #{if defined?($api_server_ip) && $api_server_ip
              "sed -i \"s@127.0.0.1@#{$api_server_ip}@g\" /vagrant/vagrant/kube.config"
            else
              "sed -i \"s@127.0.0.1@$(cat /vagrant/vagrant/ip_master)@g\" /vagrant/vagrant/kube.config"
            end}
            # sed -i "s@127.0.0.1@k8s-api.k8s.lan@g" /vagrant/vagrant/kube.config
            cp /var/lib/rancher/rke2/server/node-token /vagrant/vagrant/k8s-token
          fi
        SHELL
      end
    end
  end

  # Configuration des workers
  (1..$workers).each do |i|
    config.vm.define "k8s-#{$cluster_name}-w#{i}" do |worker|
      worker.vm.box = $vm_box
      worker.vm.box_check_update = $box_check_update
      worker.vm.hostname = "k8s-#{$cluster_name}-w#{i}"
      # IP fixe dans la plage .60-.99 (évite conflit avec MetalLB .200-.250)
      worker.vm.network "private_network", ip: "#{$network_prefix}.#{60 + i - 1}"

      worker.vm.provider "libvirt" do |domain|
        domain.default_prefix = ""
        domain.cpus = $worker_cpu
        domain.memory = $worker_memory
        domain.machine_virtual_size = $worker_disk
        domain.keymap = $keymap
        # Nested virtualization for Kata Containers
        domain.nested = true
        domain.cpu_mode = "host-passthrough"

        # Disque de stockage supplémentaire (pour Longhorn)
        if defined?($storage_disk_enabled) && $storage_disk_enabled
          domain.storage :file,
            :size => $storage_disk_size,
            :device => 'vdb',
            :bus => 'virtio',
            :type => 'raw'
        end
      end

      worker.vm.provision "shell", path: "scripts/install_common.sh"
      worker.vm.provision "shell", path: "scripts/install_worker.sh"
    end
  end
end
