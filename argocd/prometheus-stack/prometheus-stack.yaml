# Add inside spec.template.metadata.annotations
# prometheus.io/scrape: "true"
# prometheus.io/port: 8080
# prometheus.io/path: /metrics
# prometheus.io/scheme: http

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: prometheus-stack
  namespace: argo-cd
spec:
  destination:
    namespace: prometheus-stack
    name: in-cluster
    # server: 'https://kubernetes.default.svc'
  source:
    chart: kube-prometheus-stack
    repoURL: 'https://prometheus-community.github.io/helm-charts'
    targetRevision: 48.2.2
    helm:
      parameters:
        ### grafana ###
        - name: grafana.ingress.enabled
          value: 'true'
        - name: grafana.ingress.hosts[0]
          value: grafana.gigix
        - name: grafana.ingress.annotations.cert-manager\.io/cluster-issuer
          value: selfsigned-cluster-issuer
        - name: grafana.ingress.tls[0].secretName
          value: grafana-cert-tls
        - name: grafana.ingress.tls[0].hosts[0]
          value: grafana.gigix
        # - name: >-
        #     grafana.service.annotations.external-dns\.alpha\.kubernetes\.io/hostname
        #   value: grafana.gigix
        - name: grafana.adminPassword
          value: prom-operator
        - name: grafana.grafana.\ini.analytics.check_for_updates
          value: 'false'
        - name: grafana.readinessProbe.timeoutSeconds
          value: '5'
        - name: grafana.sidecar.dashboards.folderAnnotation
          value: grafana_dashboard_folder
        - name: grafana.sidecar.dashboards.provider.foldersFromFilesStructure
          value: 'true'
        # - name: grafana.sidecar.dashboards.searchNamespace
        #   value: ALL
        # - name: grafana.service.type
        #   value: LoadBalancer
        ### prometheus ###
        - name: prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName
          value: longhorn
        - name: prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.accessModes[0]
          value: ReadWriteOnce
        - name: prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage
          value: 5Gi
        - name: prometheus.ingress.enabled
          value: 'true'
        - name: prometheus.ingress.hosts[0]
          value: prometheus.gigix
        - name: prometheus.ingress.tls[0].secretName
          value: prometheus-cert-tls
        - name: prometheus.ingress.tls[0].hosts[0]
          value: prometheus.gigix
        # - name: >-
        #     prometheus.service.annotations.external-dns\.alpha\.kubernetes\.io/hostname
        #   value: prometheus.gigix
        - name: prometheus.ingress.annotations.cert-manager\.io/cluster-issuer
          value: selfsigned-cluster-issuer
        # - name: server.service.type
        #   value: LoadBalancer
        ### alertmanager ###
        - name: alertmanager.ingress.enabled
          value: 'true'
        - name: alertmanager.ingress.hosts[0]
          value: alertmanager.gigix
        - name: alertmanager.ingress.tls[0].secretName
          value: alertmanager-cert-tls
        - name: alertmanager.ingress.tls[0].hosts[0]
          value: alertmanager.gigix
        - name: alertmanager.ingress.annotations.cert-manager\.io/cluster-issuer
          value: selfsigned-cluster-issuer
        # - name: >-
        #     alertmanager.service.annotations.external-dns\.alpha\.kubernetes\.io/hostname
        #   value: alertmanager.gigix
        # - name: alertmanager.service.type
        #   value: LoadBalancer
        ### pushgateway ###
        # - name: pushgateway.ingress.enabled
        #   value: 'true'
        # - name: pushgateway.ingress.hosts[0]
        #   value: pushgateway.gigix
        # - name: pushgateway.ingress.annotations.cert-manager\.io/cluster-issuer
        #   value: selfsigned-cluster-issuer
        # - name: >-
        #     pushgateway.service.annotations.external-dns\.alpha\.kubernetes\.io/hostname
        #   value: pushgateway.gigix
        # - name: pushgateway.service.type
        #   value: LoadBalancer
        ### kubeControllerManager ###
        # - name: kubeControllerManager.endpoints[0]
        #   value: 192.168.122.122.x
        # - name: kubeControllerManager.service.port
        #   value: 10257
        # - name: kubeControllerManager.service.targetPort
        #   value: 10257
        # - name: kubeControllerManager.serviceMonitor.https
        #   value: "true"
        # - name: kubeControllerManager.serviceMonitor.insecureSkipVerify
        #   value: "true"
        ### kubeScheduler ###
        # - name: kubeScheduler.endpoints[0]
        #   value: 192.168.122.122.x
        # - name: kubeScheduler.service.port
        #   value: 10259
        # - name: kubeScheduler.service.targetPort
        #   value: 10259
        # - name: kubeScheduler.serviceMonitor.https
        #   value: "true"
        # - name: kubeScheduler.serviceMonitor.insecureSkipVerify
        #   value: "true"
        ### kubeProxy ###
        # - name: kubeProxy.endpoints[0]
        #   value: 192.168.122.122.x
        # - name: kubeProxy.service.port
        #   value: 10249
        # - name: kubeProxy.service.targetPort
        #   value: 10249
  project: default
  syncPolicy:
    syncOptions:
      # - Replace=true # use as workaroud (not recommended) => https://blog.ediri.io/kube-prometheus-stack-and-argocd-23-how-to-remove-a-workaround
      - CreateNamespace=true
      - PruneLast=true
      - ServerSideApply=true
