---
# Configuration Rook-Ceph - Dev
# Single node, single replica, minimal resources
environment: dev
appName: rook

# Dashboard configuration for Grafana
dashboard:
  folder: "Rook-Ceph"

rook:
  # Operator configuration
  operator:
    version: "v1.18.9"
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "2000m"
        memory: "512Mi"

  # Cluster configuration
  cluster:
    version: "v1.18.9"
    cephVersion: "19.2.0"
    osdPoolDefaultSize: 1
    mon:
      count: 1
      allowMultiplePerNode: true
    mgr:
      count: 1
      allowMultiplePerNode: true
    # Ceph daemon resources (from kubectl top)
    resources:
      mon:
        requests:
          cpu: "25m"
          memory: "100Mi"
        limits:
          cpu: "500m"
          memory: "1Gi"
      osd:
        requests:
          cpu: "20m"
          memory: "750Mi"
        limits:
          cpu: "2"
          memory: "4Gi"
      mgr:
        requests:
          cpu: "25m"
          memory: "512Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      crashcollector:
        requests:
          cpu: "5m"
          memory: "10Mi"
        limits:
          cpu: "100m"
          memory: "64Mi"
      exporter:
        requests:
          cpu: "5m"
          memory: "16Mi"
        limits:
          cpu: "100m"
          memory: "64Mi"
      logcollector:
        requests:
          cpu: "5m"
          memory: "10Mi"
        limits:
          cpu: "100m"
          memory: "64Mi"

  # Storage configuration (OSDs)
  storage:
    deviceFilter: "^vdb$"

  # CSI configuration
  csi:
    provisionerReplicas: 1

  # Block Pool (RBD)
  blockPool:
    replicaSize: 1

  # CephFS (Optional shared filesystem)
  cephfs:
    enabled: false
    replicaSize: 1

  # Object Store (S3 compatible)
  objectStore:
    enabled: true
    replicaSize: 1
    gateway:
      instances: 1
      # RGW resources (actual: ~6m CPU, ~120Mi mem)
      resources:
        requests:
          cpu: "10m"
          memory: "150Mi"
        limits:
          cpu: "1"
          memory: "512Mi"

  # Ceph Dashboard
  dashboard:
    enabled: true

# Sync policy
syncPolicy:
  automated:
    enabled: true
    prune: true
    selfHeal: true
  # ServerSideApply=true avoids "Too long" annotation errors on CRDs
  syncOptions:
    - CreateNamespace=true
    - ServerSideApply=true
    - PruneLast=true
