---
# Configuration Prometheus-Stack - Dev
environment: dev
appName: prometheus-stack

prometheusStack:
  version: "80.14.3"
  prometheus:
    # Persistence (requires features.storage.enabled)
    persistence:
      enabled: true
      size: "2Gi"

    # Retention
    retention: "2d"

    # Replicas (dev: 1)
    replicas: 1

    # Resources (actual: 29m/539Mi)
    resources:
      requests:
        cpu: "25m"
        memory: "650Mi"
      limits:
        cpu: "1000m"
        memory: "2Gi"

    # Ingress (dev: disabled - using Gateway API HTTPRoute)
    ingress:
      enabled: false
      hostname: "prometheus.{{ .common.domain }}"

    # Remote write receiver (for external metrics)
    enableRemoteWriteReceiver: false

  grafana:
    # Admin credentials managed via KSOPS encrypted secrets
    # See: secrets/secret-dev.yaml

    # ===================
    # OIDC Configuration (Keycloak)
    # ===================
    # Note: Les URLs OIDC sont construites dynamiquement dans l'ApplicationSet
    # en utilisant common.domain et le realm ci-dessous
    oidc:
      enabled: true
      name: "Keycloak"
      realm: "k8s"  # Keycloak realm name - URLs construites via {{ .common.domain }}
      allowSignUp: true
      autoLogin: true
      clientId: "grafana"
      scopes: "openid email profile groups"
      # Mapping des groupes Keycloak vers les roles Grafana
      # admins -> Admin, developers -> Editor, autres -> Viewer
      roleAttributePath: "contains(groups[*], 'admins') && 'Admin' || contains(groups[*], 'developers') && 'Editor' || 'Viewer'"
      allowAssignGrafanaAdmin: true

    # Replicas (dev: 1)
    replicas: 1

    # Persistence (requires features.storage.enabled)
    persistence:
      enabled: true
      size: "1Gi"

    # Resources (actual: 14m/308Mi)
    resources:
      requests:
        cpu: "50m"
        memory: "256Mi"
      limits:
        cpu: "500m"
        memory: "1Gi"

    # Ingress (dev: disabled - using Gateway API HTTPRoute)
    ingress:
      enabled: false
      hostname: "grafana.{{ .common.domain }}"

    # Grafana.ini configuration
    grafanaIni:
      analytics:
        check_for_updates: false

      # Alerting configuration
      unified_alerting:
        enabled: true      # Nouveau système d'alerting (Grafana 8+, obligatoire depuis Grafana 12+)

    # Sidecar configuration
    sidecar:
      # Sidecar resources (actual: ~2m CPU, ~91Mi mem)
      resources:
        requests:
          cpu: "5m"
          memory: "100Mi"
        limits:
          cpu: "100m"
          memory: "256Mi"
      # Sidecar dashboards configuration
      # Note: folderAnnotation est géré par ApplicationSet via features.monitoring.dashboard.folderAnnotation
      dashboards:
        enabled: true
        # folderAnnotation: géré par ApplicationSet (default: features.monitoring.dashboard.folderAnnotation)
        folder: "Prometheus Stack"  # Nom du dossier Grafana pour les dashboards
        provider:
          foldersFromFilesStructure: true

    # Readiness probe
    readinessProbe:
      timeoutSeconds: 5

    # ServiceMonitor for Grafana metrics
    serviceMonitor:
      enabled: true
      labels:
        release: "{{ .features.monitoring.release }}"

  alertmanager:
    # Alertmanager (dev: enabled)
    enabled: true

    # Replicas (dev: 1)
    replicas: 1

    # Resources (actual: 1m/19Mi)
    resources:
      requests:
        cpu: "5m"
        memory: "25Mi"
      limits:
        cpu: "100m"
        memory: "128Mi"

    # Ingress (dev: disabled - using Gateway API HTTPRoute)
    ingress:
      enabled: false
      hostname: "alertmanager.{{ .common.domain }}"

    # Persistence (requires features.storage.enabled)
    persistence:
      enabled: true
      size: "1Gi"

  pushgateway:
    # Pushgateway (dev: disabled)
    enabled: false

    # Resources (dev: minimal)
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "256Mi"

    # ServiceMonitor
    serviceMonitor:
      enabled: true
      namespace: "monitoring"

  # Prometheus Operator (usage: 6m/27Mi)
  prometheusOperator:
    resources:
      requests:
        cpu: "10m"
        memory: "32Mi"
      limits:
        cpu: "200m"
        memory: "128Mi"
    # Config reloader resources (actual: ~1m CPU, ~14Mi mem)
    configReloader:
      resources:
        requests:
          cpu: "5m"
          memory: "20Mi"
        limits:
          cpu: "100m"
          memory: "128Mi"

  # Node Exporter resources (actual: ~1m CPU, ~13Mi mem)
  nodeExporter:
    resources:
      requests:
        cpu: "5m"
        memory: "20Mi"
      limits:
        cpu: "200m"
        memory: "128Mi"

  # Kube State Metrics (usage: 4m/21Mi)
  kubeStateMetrics:
    resources:
      requests:
        cpu: "10m"
        memory: "32Mi"
      limits:
        cpu: "200m"
        memory: "128Mi"

syncPolicy:
  automated:
    enabled: true
    prune: true
    selfHeal: true
